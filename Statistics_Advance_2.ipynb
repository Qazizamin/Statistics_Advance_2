{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. Explain the properties of the F-distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The F-distribution is a probability distribution that arises frequently in statistical analyses, particularly in the context of variance analysis and hypothesis testing.\n",
    "\n",
    "- Definition\n",
    "The F-distribution is the distribution of the ratio of two independent chi-squared variables divided by their respective degrees of freedom. Specifically, if X and Y are independent chi-squared random variables with d1 and d2 degrees of freedom, respectively, then the variable follows an F-distribution with d1 d2 degrees of freedom.\n",
    "\n",
    "- 1. Shape:\n",
    "- The F-distribution is right-skewed, especially for smaller degrees of freedom. As the degrees of freedom increase, the distribution becomes more symmetric. It does not have negative values; the distribution is defined for positive values only.\n",
    "\n",
    "- 2. Degrees of Freedom: The F-distribution is characterized by two sets of degrees of freedom: d1 (numerator degrees of freedom) and d2 (denominator degrees of freedom). These degrees of freedom typically correspond to the number of groups being compared and the total number of observations, respectively.\n",
    "\n",
    "- 3. Applications: The F-distribution is commonly used in:\n",
    "- ANOVA (Analysis of Variance): To compare the variances between different groups.\n",
    "- Regression Analysis: To test the significance of models by comparing the variances explained by the model versus the residual variance.\n",
    "- Hypothesis Testing: Particularly when comparing the variances of two populations.\n",
    "\n",
    "- 4. Critical Values:\n",
    "- F-distribution tables are used to find critical values for hypothesis tests, where the significance level (α) determines the threshold for rejecting the null hypothesis. the critical values depend on the degrees of freedom and the desired significance level.\n",
    "\n",
    "- 5. Non-Normality: The F-distribution can be affected by the underlying distributions of the sample data, particularly if the sample sizes are small or if the data do not meet the assumptions of normality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2. In which types of statistical tests is the F-distribution used, and why is it appropriate for these tests? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The F-distribution is used primarily in several types of statistical tests that involve comparing variances across groups. Here are the key statistical tests where the F-distribution is appropriate:\n",
    "\n",
    "- 1. Analysis of Variance (ANOVA):\n",
    "- Purpose: ANOVA is used to compare the means of three or more groups to determine if at least one group mean is significantly different from the others.\n",
    "- Use of F-distribution: The test calculates the ratio of the variance between the group means to the variance within the groups. If the null hypothesis (that all group means are equal) is true, the ratio should be close to 1, following an F-distribution under the null hypothesis.\n",
    "- Why Appropriate: ANOVA is based on the assumption that the populations from which the samples are drawn have equal variances (homogeneity of variance). The F-distribution is appropriate because it compares the ratio of variances.\n",
    "\n",
    "- 2. Regression Analysis:\n",
    "- Purpose: In multiple regression analysis, the F-test assesses the overall significance of the model by comparing the variance explained by the model to the variance not explained (the residual variance).\n",
    "- Use of F-distribution: The F-statistic is computed as the ratio of the mean square of the regression (explained variance) to the mean square of the residuals (unexplained variance).\n",
    "- Why Appropriate: It helps to determine if at least one predictor variable has a statistically significant relationship with the dependent variable. The F-distribution is used because it describes the distribution of the ratio of two independent chi-squared variables.\n",
    "\n",
    "- 3. Comparing Two Variances (F-test for Equality of Variances)\n",
    "- Purpose: This test compares the variances of two populations to determine if they are significantly different.\n",
    "- Use of F-distribution: The F-statistic is calculated as the ratio of the two sample variances. If the null hypothesis (that the variances are equal) is true, this ratio follows an F-distribution.\n",
    "- Why Appropriate: It is suitable because the F-distribution is derived from the ratio of variances, making it a natural choice for this comparison.\n",
    "\n",
    "- 4. Multivariate Analysis of Variance (MANOVA)\n",
    "- Purpose: MANOVA is an extension of ANOVA that allows for the comparison of multiple dependent variables simultaneously.\n",
    "- Use of F-distribution: Similar to ANOVA, it uses F-statistics to test the hypothesis that the means of the groups are equal across multiple dependent variables.\n",
    "- Why Appropriate: It leverages the F-distribution to handle multiple outcomes and assess overall group differences while accounting for the correlations between the dependent variables.\n",
    "\n",
    "- 5. General Linear Models (GLM)\n",
    "- Purpose: GLM includes various statistical models where the response variable's distribution can be exponential family distributions.\n",
    "- Use of F-distribution: F-tests are used to compare nested models (e.g., to see if adding predictors significantly improves the model fit).\n",
    "- Why Appropriate: The F-distribution provides a way to assess whether the addition of one or more predictors significantly increases explained variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3. What are the key assumptions required for conducting an F-test to compare the variances of two populations? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- When conducting an F-test to compare the variances of two populations, several key assumptions must be satisfied to ensure the validity of the test results. Here are the primary assumptions:\n",
    "\n",
    "- 1. Independence of Samples: The samples drawn from each population must be independent of each other. This means that the selection of individuals in one sample should not influence the selection of individuals in the other sample.\n",
    "\n",
    "- 2. Normality: The populations from which the samples are drawn should be normally distributed. While the F-test is somewhat robust to violations of this assumption, especially with larger sample sizes (thanks to the Central Limit Theorem), it is still preferable to have normally distributed data, particularly for smaller samples.\n",
    "\n",
    "- 3. Homogeneity of Variances: The F-test specifically tests the null hypothesis that the two populations have equal variances. Therefore, the assumption is that the variances of the two populations should be equal. If this assumption is violated, the F-test may not produce valid results.\n",
    "\n",
    "- 4. Random Sampling: The samples should be randomly selected from their respective populations. This ensures that the samples are representative of the populations being studied.\n",
    "\n",
    "- 5. Continuous Data: The data should be continuous. The F-test is typically not appropriate for categorical data, as variances for categorical outcomes are not meaningful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4. What is the purpose of ANOVA, and how does it differ from a t-test? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ANOVA (Analysis of Variance) and the t-test are both statistical methods used to compare means, but they serve different purposes and are used in different contexts. \n",
    "\n",
    "### Purpose of ANOVA:\n",
    "- 1. Comparison of Means: ANOVA is designed to compare the means of three or more groups to determine if at least one group mean is significantly different from the others. It assesses whether the variability between the group means is greater than the variability within the groups.\n",
    "\n",
    "- 2. Hypothesis Testing: The null hypothesis for ANOVA states that all group means are equal, while the alternative hypothesis states that at least one group mean is different.\n",
    "\n",
    "- 3. Variance Analysis: ANOVA analyzes variance to determine how much of the total variability in the data can be attributed to the differences between group means, as opposed to within-group variability.\n",
    "\n",
    "### Types of ANOVA\n",
    "- 1. One-Way ANOVA: Compares means across one independent variable with multiple levels (e.g., testing the effect of different diets on weight loss).\n",
    "- 2. Two-Way ANOVA: Compares means across two independent variables and can assess the interaction between them (e.g., testing the effects of diet and exercise on weight loss).\n",
    "\n",
    "\n",
    "### Differences Between ANOVA and t-Test\n",
    "\n",
    "- 1. Number of Groups:\n",
    "- t-Test: Typically compares the means of two groups (e.g., comparing test scores between two classes).\n",
    "- ANOVA: Compares the means of three or more groups (e.g., comparing test scores across three different classes).\n",
    "\n",
    "- 2. Null Hypothesis:\n",
    "- t-Test: The null hypothesis states that the means of the two groups are equal.\n",
    "- ANOVA: The null hypothesis states that all group means are equal (not just two).\n",
    "\n",
    "- 3. Type of Data:\n",
    "- t-Test: Suitable for situations where the data is normally distributed and has approximately equal variances.\n",
    "- ANOVA: Also assumes normality and homogeneity of variances but can handle more complex experimental designs.\n",
    "\n",
    "- 4. Output:\n",
    "- t-Test: Produces a t-statistic and p-value to determine significance.\n",
    "- ANOVA: Produces an F-statistic and p-value, comparing variance between groups to variance within groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5. Explain when and why you would use a one-way ANOVA instead of multiple t-tests when comparing more than two groups. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When to Use One-Way ANOVA:\n",
    "### You should use one-way ANOVA when:\n",
    "- You have three or more independent groups to compare.\n",
    "- You want to control for Type I error across multiple comparisons.\n",
    "- You aim to determine if there is any significant effect of an independent variable on a dependent variable.\n",
    "\n",
    "### When comparing the means of more than two groups, a one-way ANOVA is preferred over conducting multiple t-tests for several important reasons:\n",
    "\n",
    "### 1. Type I Error Rate Control:\n",
    "- One-way ANOVA: By using a single test to compare all groups simultaneously, one-way ANOVA controls the overall Type I error rate. When conducting multiple t-tests, the probability of incorrectly rejecting at least one null hypothesis (Type I error) increases with each additional test.\n",
    "- Multiple t-Tests: If you perform multiple t-tests (for example, if you have four groups and perform six t-tests to compare every possible pair), the cumulative risk of making a Type I error becomes significant. For instance, with a significance level of 0.05, running multiple tests can inflate the overall alpha level beyond 0.05, leading to potentially misleading conclusions.\n",
    "\n",
    "### 2. Efficiency and Simplicity:\n",
    "- One-way ANOVA: It is more efficient to use one test instead of multiple tests. With one-way ANOVA, you can analyze all group means in a single analysis and get a comprehensive view of the differences.\n",
    "- Multiple t-Tests: Conducting several t-tests can be cumbersome and complex, especially as the number of groups increases.\n",
    "\n",
    "### 3. Identifying Overall Differences:\n",
    "- One-way ANOVA: It provides a single F-statistic that indicates whether there are any significant differences among the group means. This allows researchers to assess the overall effect of the independent variable on the dependent variable.\n",
    "- Multiple t-Tests: While t-tests can show whether specific pairs of means differ, they do not provide a broader perspective on whether at least one group differs from the others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6. Explain how variance is partitioned in ANOVA into between-group variance and within-group variance. How does this partitioning contribute to the calculation of the F-statistic?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In ANOVA (Analysis of Variance), the total variance observed in the data is partitioned into two components: between-group variance and within-group variance. This partitioning is fundamental to understanding how ANOVA tests whether there are significant differences among group means. Here’s a detailed explanation of how this partitioning works and how it contributes to the calculation of the F-statistic.\n",
    "\n",
    "- 1. Total Variance:\n",
    "- The total variance in the data is measured by the total sum of squares (SST), which quantifies the total variability of the observations around the overall mean (Y(bar)). It is calculated as: \n",
    "- SST= ∑(Yi − Y(bar))^2\n",
    "- where Yi is an individual observation and Y(bar) is the overall mean of all observations.\n",
    "\n",
    "- 2. Between-Group Variance (SSB):\n",
    "- Between-group variance (also known as the sum of squares between groups, SSB) measures the variability attributed to the differences between the group means. It indicates how much the group means differ from the overall mean.\n",
    " \n",
    " - 3. Within-Group Variance (SSW):\n",
    "- Within-group variance (also known as the sum of squares within groups, SSW) measures the variability of observations within each group. It assesses how much individual observations deviate from their respective group means.\n",
    "\n",
    "- 4. Partitioning the Total Variance:\n",
    "- The total variance can be partitioned as follows: SST = SSB + SSW \n",
    "- This equation shows how the total variability in the data is composed of variability due to group differences (between-group) and variability within the groups themselves.\n",
    "\n",
    "- 5. Calculating the F-Statistic:\n",
    "- The F-statistic is calculated to assess whether the between-group variance is significantly greater than the within-group variance.\n",
    "- Calculate the mean square between (MSB):\n",
    "- Calculate the mean square within (MSW):\n",
    "- Calculate the F-statistic: F = MSW/MSB\n",
    "​\t\n",
    " - 6. Interpretation of the F-Statistic:\n",
    "- The F-statistic represents the ratio of the variance explained by the group differences to the variance within the groups. A larger F-value suggests that the means of the groups are significantly different, as it indicates that the variability among group means (due to treatment effects) is greater than the variability within the groups (due to random error)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7. Compare the classical (frequentist) approach to ANOVA with the Bayesian approach. What are the key differences in terms of how they handle uncertainty, parameter estimation, and hypothesis testing? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The classical (frequentist) approach and the Bayesian approach to ANOVA offer different philosophies and methodologies for handling statistical analysis. Here are the key differences between these two approaches in terms of uncertainty, parameter estimation, and hypothesis testing:\n",
    "\n",
    "# 1. Handling Uncertainty:\n",
    "### Frequentist Approach:\n",
    "- Probability Interpretation: In the frequentist framework, probability is interpreted as the long-run frequency of events. This means that parameters are considered fixed but unknown quantities, and the uncertainty is expressed in terms of confidence intervals or p-values.\n",
    "- Confidence Intervals: Confidence intervals provide a range of values for an estimate that would contain the true parameter a certain percentage of the time (e.g., 95%). However, they do not provide a direct probability that the parameter falls within the interval for a specific sample.\n",
    "\n",
    "### Bayesian Approach:\n",
    "- Probability as Belief: The Bayesian interpretation treats probability as a degree of belief about an event or parameter. This allows for more direct statements about uncertainty regarding parameters.\n",
    "- Credible Intervals: Bayesian credible intervals provide a range of values within which the parameter lies with a specified probability, directly reflecting the uncertainty based on the observed data and prior beliefs.\n",
    "\n",
    "\n",
    "# 2. Parameter Estimation\n",
    "\n",
    "### Frequentist Approach:\n",
    "- Point Estimates: Parameters are estimated using methods like maximum likelihood estimation (MLE) or method of moments. These estimates are fixed values and do not incorporate prior beliefs or information.\n",
    "- No Prior Information: The frequentist approach does not use prior distributions, focusing solely on the data from the current study.\n",
    "\n",
    "### Bayesian Approach:\n",
    "- Posterior Distributions: Parameters are estimated using a prior distribution combined with the likelihood of the observed data to form a posterior distribution. This allows for continuous updating of beliefs about the parameter as new data becomes available.\n",
    "- Incorporation of Prior Knowledge: The Bayesian method can incorporate prior information or beliefs through the prior distribution, allowing for a more nuanced estimation process.\n",
    "\n",
    "\n",
    "# 3. Hypothesis Testing\n",
    "\n",
    "### Frequentist Approach:\n",
    "- Null Hypothesis Significance Testing (NHST): The frequentist framework relies on NHST, where the null hypothesis is tested against an alternative hypothesis. Decisions are made based on p-values, which indicate the probability of observing the data (or more extreme) if the null hypothesis is true.\n",
    "- Fixed Significance Level: The significance level (e.g., alpha = 0.05) is pre-determined, and if the p-value is below this threshold, the null hypothesis is rejected.\n",
    "\n",
    "### Bayesian Approach:\n",
    "- Bayes Factors: In Bayesian hypothesis testing, Bayes factors are used to compare the evidence for different hypotheses. A Bayes factor quantifies how much more likely the observed data is under one hypothesis compared to another.\n",
    "- Direct Comparison of Hypotheses: Bayesian methods allow for direct comparison between the null and alternative hypotheses, incorporating prior beliefs about each hypothesis.\n",
    "\n",
    "\n",
    "# 4. Model Complexity and Interpretation\n",
    "\n",
    "### Frequentist Approach:\n",
    "- Model Simplicity: Frequentist methods tend to focus on simpler models and rely on asymptotic properties of estimators for inference.\n",
    "- Interpretation of Results: Results are typically interpreted in terms of rejection or non-rejection of the null hypothesis, which can sometimes lead to binary conclusions that may overlook nuances in the data.\n",
    "\n",
    "### Bayesian Approach:\n",
    "- Flexibility and Complexity: Bayesian models can accommodate more complex structures and hierarchical models, allowing for richer interpretations.\n",
    "- Interpretation of Results: Bayesian results provide a more intuitive understanding of the parameter estimates and their uncertainty, as they allow for probabilistic statements about parameter values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q8. Question: You have two sets of data representing the incomes of two different professions: • Profession A: [48, 52, 55, 60, 62] Profession B: [45, 50, 55, 52, 47] Perform an F-test to determine if the variances of the two professions incomes are equal. What are your conclusions based on the F-test? Task: Use Python to calculate the F-statistic and p-value for the given data. Objective: Gain experience in performing F-tests and interpreting the results in terms of variance comparison. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance of Profession A: 32.80\n",
      "Variance of Profession B: 15.70\n",
      "F-statistic: 2.09\n",
      "Critical F-value at alpha = 0.05: 6.39\n",
      "Fail to reject the null hypothesis: The variances are equal.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Data for Profession A and B\n",
    "profession_a = np.array([48, 52, 55, 60, 62])\n",
    "profession_b = np.array([45, 50, 55, 52, 47])\n",
    "\n",
    "# Calculate sample variances\n",
    "var_a = np.var(profession_a, ddof=1)  # Sample variance for A\n",
    "var_b = np.var(profession_b, ddof=1)  # Sample variance for B\n",
    "\n",
    "# Calculate the F-statistic\n",
    "F_statistic = var_a / var_b\n",
    "\n",
    "# Degrees of freedom\n",
    "df_a = len(profession_a) - 1\n",
    "df_b = len(profession_b) - 1\n",
    "\n",
    "# Calculate the critical value for F-distribution at alpha = 0.05\n",
    "alpha = 0.05\n",
    "F_critical = stats.f.ppf(1 - alpha, df_a, df_b)\n",
    "\n",
    "# Print results\n",
    "print(f\"Variance of Profession A: {var_a:.2f}\")\n",
    "print(f\"Variance of Profession B: {var_b:.2f}\")\n",
    "print(f\"F-statistic: {F_statistic:.2f}\")\n",
    "print(f\"Critical F-value at alpha = {alpha}: {F_critical:.2f}\")\n",
    "\n",
    "# Conclusion\n",
    "if F_statistic < F_critical:\n",
    "    print(\"Fail to reject the null hypothesis: The variances are equal.\")\n",
    "else:\n",
    "    print(\"Reject the null hypothesis: The variances are significantly different.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q9. Question: Conduct a one-way ANOVA to test whether there are any statistically significant differences in average heights between three different regions with the following data: • Region A: [160, 162, 165, 158, 164] • Region B: [172, 175, 170, 168, 174] • Region C: [180, 182, 179, 185, 183] • Task: Write Python code to perform the one-way ANOVA and interpret the results. • Objective: Learn how to perform one-way ANOVA using Python and interpret F-statistic and p-value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-statistic: 67.87\n",
      "P-value: 0.0000\n",
      "Reject the null hypothesis: There are significant differences in average heights among the regions.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Heights data for the three regions\n",
    "region_a = np.array([160, 162, 165, 158, 164])\n",
    "region_b = np.array([172, 175, 170, 168, 174])\n",
    "region_c = np.array([180, 182, 179, 185, 183])\n",
    "\n",
    "# Conducting one-way ANOVA\n",
    "f_statistic, p_value = stats.f_oneway(region_a, region_b, region_c)\n",
    "\n",
    "# Print results\n",
    "print(f\"F-statistic: {f_statistic:.2f}\")\n",
    "print(f\"P-value: {p_value:.4f}\")\n",
    "\n",
    "# Conclusion based on a significance level of 0.05\n",
    "alpha = 0.05\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis: There are significant differences in average heights among the regions.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: No significant differences in average heights among the regions.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
